{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji04wRIx6QxY"
      },
      "source": [
        "### Ingestion and calling the Claude API\n",
        "The best way to pass Claude charts and graphs is to take advantage of its vision capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "uCvCR_-K6QxY"
      },
      "outputs": [],
      "source": [
        "# # pip install -qUr requirements.txt\n",
        "# !pip install pdf2image==1.17.0\n",
        "# !pip install PyMuPDF==1.24.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tapkAv346QxZ"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "from datetime import datetime\n",
        "from IPython.display import Image\n",
        "from botocore.exceptions import ClientError\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "session = boto3.Session()\n",
        "region = session.region_name\n",
        "\n",
        "#modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
        "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
        "\n",
        "print(f'Using modelId: {modelId}')\n",
        "print('Using region: ', region)\n",
        "\n",
        "bedrock_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "k9vKMsqd6QxZ"
      },
      "outputs": [],
      "source": [
        "def get_completion(messages):\n",
        "    converse_api_params = {\n",
        "        \"modelId\": modelId,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "    response = bedrock_client.converse(**converse_api_params)\n",
        "    # Extract the generated text content from the response\n",
        "    return response['output']['message']['content'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Kxs2KZVx6QxZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def formattedtime(seconds):\n",
        "    #print(f\"formattedtime({seconds})\")\n",
        "    final_time = time.strftime(\"%H:%M:%S\", time.gmtime(float(seconds)))\n",
        "    return f\"{final_time}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PfYSQ3HT6QxZ"
      },
      "outputs": [],
      "source": [
        "question = \"Test Question Here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "B_kLb1Y46QxZ"
      },
      "outputs": [],
      "source": [
        "# Now let's pass the first 20 of these images (in order) to Claude at once and ask it a question about the deck.\n",
        "# Why 20? Currently, the Anthropic API only allows you to pass in a maximum of 20 images.\n",
        "# While this number will likely increase over time, we have some helpful tips for how to manage it later in this recipe.\n",
        "\n",
        "\n",
        "# question = \"What was Twilio y/y revenue growth for fiscal year 2023?\"\n",
        "\n",
        "prompt = f\"\"\"You are a question answering assistant. Answer the given question if its evidence\n",
        "is present in the input image in the <answer> tab else just output <answer> don't no </output> without adding\n",
        "any additional text strictly.\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "<instructions>\n",
        "- If answer is just not present just output <answer> don't know </output> and don't add any additional\n",
        "text strictly in this case since you will be penalised for additional text .\n",
        "- Strictly don't say things like 'I'm sorry, but I don't ' or\n",
        "'I don't see any relevant information in the given images to answer this question.'\n",
        "'I do not have enough information to answer this question'\n",
        "etc. just say 'don't know'\n",
        "</instructions>\n",
        "\"\"\"\n",
        "\n",
        "#question = \"What was the non-GAAP gross margin?\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "yIhi2LFj6Qxa"
      },
      "outputs": [],
      "source": [
        "image_dir = f\"/data/split_video\"\n",
        "image_path_list = []\n",
        "answer_total = \"\"\n",
        "counter = 0\n",
        "from tqdm.notebook import tqdm\n",
        "for images in tqdm(os.listdir(image_dir)):\n",
        "    # print(images)\n",
        "    if images == \".ipynb_checkpoints\":\n",
        "        continue\n",
        "    with open(f\"{image_dir}/{images}\", \"rb\") as f:\n",
        "        img = f.read()\n",
        "\n",
        "    content = [{\"image\": {\"format\": 'png', \"source\": {\"bytes\": img}}}]\n",
        "\n",
        "    # Append the question to our images\n",
        "    content.append({\"text\": prompt})\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": 'user',\n",
        "            \"content\": content\n",
        "        },\n",
        "        {\n",
        "            \"role\": 'assistant',\n",
        "            \"content\": [{\"text\":\"<answer>:\"}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    answer = get_completion(messages)\n",
        "    time_stamp = int(images.split(\"_\")[-1][:-4])\n",
        "\n",
        "\n",
        "    start_time = formattedtime(time_stamp)\n",
        "\n",
        "    end_time = formattedtime(time_stamp + 60)\n",
        "    file = images[:images.find(\".mp4\")+4]\n",
        "\n",
        "\n",
        "    answer_total = f\"\"\"{answer_total}\\n\\n image file - {file} \\n start time - {start_time}\n",
        "\n",
        "    end time - {end_time} \\n \\n\n",
        "\n",
        "    Answer :- {answer}\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FJH7J_6K6Qxa"
      },
      "outputs": [],
      "source": [
        "# file = \"Lecture_20___Bayesian_Network-mod04lec20.mp4_49080.jpg\"\n",
        "# time_stamp = int(file.split(\"_\")[-1][:-4])\n",
        "\n",
        "\n",
        "# start_time = formattedtime(time_stamp)\n",
        "# end_time = formattedtime(time_stamp + 60)\n",
        "\n",
        "# start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lwea58wc6Qxa"
      },
      "outputs": [],
      "source": [
        "answer_total = answer_total.replace(\"don't know\",\"\")\n",
        "answer_total = answer_total.replace(\"</answer>\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "SzWgRV3P6Qxa"
      },
      "outputs": [],
      "source": [
        "print(answer_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vclsUsCW6Qxa"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt_final = f\"\"\"The given context has answer tags since the output is generated from previous conversations.\n",
        "It may contain a lot of text saying answer is not present in the context and you need to ignore them.\n",
        "Answer might be present in some of the tags. Use them to answer the question.\n",
        "Here is the  context:\n",
        "<context>\n",
        "{answer_total}\n",
        "</context>\n",
        "Answer the question\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "Remember to enclose the answer in <answer> tags\n",
        "Also generate start_time, end_time and image_file in start_time,end_time and image_file tags respectively which is the source of answer.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": 'user',\n",
        "        \"content\": [\n",
        "            {\"text\": prompt}\n",
        "\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "answer = get_completion(messages)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GHAYQDPM6Qxb"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzsIJjCB6Qxb"
      },
      "source": [
        "---\n",
        "\n",
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0raeiQYQ6Qxb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# with open(\"../app/QA_PAIR.generated_output.jsonl\", 'r') as my_file:\n",
        "#     qa_pair = json.loads(my_file.read())\n",
        "\n",
        "import json\n",
        "\n",
        "data = []\n",
        "with open(\"../app/generated_output.jsonl\", 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# scores_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d_6Hrhg16Qxb"
      },
      "outputs": [],
      "source": [
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6gJa5FHm6Qxc"
      },
      "outputs": [],
      "source": [
        "claude_precision_list,claude_recall_list,claude_fmeasure_list  = [] , [], []\n",
        "llama_precision_list,llama_recall_list,llama_fmeasure_list  = [] , [], []\n",
        "multi_modal_precision_list,multi_modal_recall_list,multi_modal_fmeasure_list  = [] , [], []\n",
        "\n",
        "for item in data:\n",
        "    claude = item[\"generated_answer_claude3\"]\n",
        "    llama3 = item[\"llama3\"]\n",
        "    multi_model = item[\"multi_model\"]\n",
        "    ground_truth = item[\"ground_truth\"]\n",
        "    score_claude = scorer.score(ground_truth,claude)[\"rouge1\"]\n",
        "    score_llama3 = scorer.score(ground_truth,llama3)[\"rouge1\"]\n",
        "    score_multi_modal = scorer.score(ground_truth,multi_model)[\"rouge1\"]\n",
        "    # print(score_multi_modal.recall)\n",
        "    claude_precision_list.append(score_claude.precision)\n",
        "    claude_recall_list.append(score_claude.recall)\n",
        "    claude_fmeasure_list.append(score_claude.fmeasure)\n",
        "    llama_precision_list.append(score_llama3.precision)\n",
        "    llama_recall_list.append(score_llama3.recall)\n",
        "    llama_fmeasure_list.append(score_llama3.fmeasure)\n",
        "    multi_modal_precision_list.append(score_multi_modal.precision)\n",
        "    multi_modal_recall_list.append(score_multi_modal.recall)\n",
        "    multi_modal_fmeasure_list.append(score_multi_modal.fmeasure)\n",
        "\n",
        "\n",
        "    #     precision_list.append(item[\"rouge1\"].precision)\n",
        "#     recall_list.append(item[\"rouge1\"].recall)\n",
        "#     fmeasure_list.append(item[\"rouge1\"].fmeasure)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WUUTdbUy6Qxc"
      },
      "outputs": [],
      "source": [
        "np.mean(claude_precision_list) , np.mean(claude_recall_list) , np.mean(claude_fmeasure_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YENNt3dm6Qxc"
      },
      "outputs": [],
      "source": [
        "np.mean(llama_precision_list) , np.mean(llama_recall_list) , np.mean(llama_fmeasure_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wYrltH9E6Qxc"
      },
      "outputs": [],
      "source": [
        "np.mean(multi_modal_precision_list) , np.mean(multi_modal_recall_list) , np.mean(multi_modal_fmeasure_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lRfj2mZ_6Qxc",
        "outputId": "9a9fcecc-1e88-4413-b70e-5eab0fef95ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_modal_recall_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ecRJHoUx6Qxd"
      },
      "outputs": [],
      "source": [
        "image_dir = f\"/data/split_video\"\n",
        "\n",
        "\n",
        "\n",
        "time_frames = 40\n",
        "\n",
        "for item in tqdm(qa_pair):\n",
        "    question = item[\"question\"]\n",
        "    ground_truth = item[\"answer\"]\n",
        "\n",
        "\n",
        "    os.system(f\"rm -r {image_dir}\")\n",
        "    os.system(f\"mkdir -p {image_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "    pathOut = image_dir\n",
        "    input_video_path =f\"/data/knowledgebase_video\"\n",
        "    count = 0\n",
        "    counter = 1\n",
        "    listing = os.listdir(input_video_path)\n",
        "    vid =\n",
        "    for vid in listing:\n",
        "        full_vid_path = f\"{input_video_path}/{vid}\"\n",
        "        vid = vid.replace(\" \",\"_\")\n",
        "        cap = cv2.VideoCapture(full_vid_path)\n",
        "        count = 0\n",
        "        counter += 1\n",
        "        success = True\n",
        "        while success:\n",
        "            success,image = cap.read()\n",
        "            # print('read a new frame:',success)\n",
        "            if success == False:\n",
        "                break\n",
        "            if count >= time_frames and count<=time_frames + 240:\n",
        "                if count % 15 == 0:\n",
        "                    #cv2.imwrite(pathOut + 'frame%d.jpg'%count,image)\n",
        "                    #print(f\"writing -- {pathOut}/{vid}_{count}.jpg\")\n",
        "                    cv2.imwrite(f\"{pathOut}/{vid}_{count}.jpg\",image)\n",
        "            count+=1\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Done this !\")\n",
        "\n",
        "\n",
        "    image_path_list = []\n",
        "    answer_total = \"\"\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for images in tqdm(os.listdir(image_dir)):\n",
        "        # print(images)\n",
        "        if images == \".ipynb_checkpoints\":\n",
        "            continue\n",
        "        with open(f\"{image_dir}/{images}\", \"rb\") as f:\n",
        "            img = f.read()\n",
        "\n",
        "        content = [{\"image\": {\"format\": 'png', \"source\": {\"bytes\": img}}}]\n",
        "\n",
        "        # Append the question to our images\n",
        "        content.append({\"text\": prompt})\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": 'user',\n",
        "                \"content\": content\n",
        "            },\n",
        "            {\n",
        "                \"role\": 'assistant',\n",
        "                \"content\": [{\"text\":\"<answer>:\"}]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        answer = get_completion(messages)\n",
        "        time_stamp = int(images.split(\"_\")[-1][:-4])\n",
        "\n",
        "\n",
        "        start_time = formattedtime(time_stamp)\n",
        "\n",
        "        end_time = formattedtime(time_stamp + 60)\n",
        "        file = images[:images.find(\".mp4\")+4]\n",
        "\n",
        "\n",
        "        answer_total = f\"\"\"{answer_total}\n",
        "\n",
        "        Answer :- {answer}\"\"\"\n",
        "\n",
        "\n",
        "        answer_total = answer_total.replace(\"don't know\",\"\")\n",
        "        answer_total = answer_total.replace(\"</answer>\",\"\")\n",
        "\n",
        "\n",
        "        prompt_final = f\"\"\"The given context has answer tags since the output is generated from previous conversations.\n",
        "        It may contain a lot of text saying answer is not present in the context and you need to ignore them.\n",
        "        Answer might be present in some of the tags. Use them to answer the question.\n",
        "        Here is the  context:\n",
        "        <context>\n",
        "        {answer_total}\n",
        "        </context>\n",
        "        Answer the question\n",
        "        <question>\n",
        "        {question}\n",
        "        </question>\n",
        "        Remember to enclose the answer in <answer> tags\n",
        "        Also generate start_time, end_time and image_file in start_time,end_time and image_file tags respectively which is the source of answer.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": 'user',\n",
        "                \"content\": [\n",
        "                    {\"text\": prompt_final}\n",
        "\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        answer = get_completion(messages)\n",
        "        print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmEFVc6U6Qxd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (Data Science 3.0)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}